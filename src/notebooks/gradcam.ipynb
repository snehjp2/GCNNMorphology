{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts/')\n",
    "# from models import model_dict\n",
    "from dataset import Galaxy10DECals\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from e2cnn import nn as e2cnn_nn\n",
    "from cnn import ConvBlock as CNN_ConvBlock\n",
    "import cnn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "\n",
    "def grad_cam(model, image, target_class=None):\n",
    "    # Store the gradients of the last convolutional layer\n",
    "    gradients = []\n",
    "    activations = []\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        gradients.append(grad_output[0])\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "\n",
    "    # Register hooks\n",
    "    last_conv_layer = list(model.children())[-2]  # assuming the last layer is a fully connected layer\n",
    "    handle_forward = last_conv_layer.register_forward_hook(forward_hook)\n",
    "    handle_backward = last_conv_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(image)\n",
    "    model.zero_grad()\n",
    "\n",
    "    # If no specific class is targeted, use the max prediction\n",
    "    if target_class is None:\n",
    "        target_class = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Backward pass\n",
    "    logits[0, target_class].backward()\n",
    "\n",
    "    # Unregister hooks\n",
    "    handle_forward.remove()\n",
    "    handle_backward.remove()\n",
    "\n",
    "    gradient = gradients[0]  # [C, H, W]\n",
    "    alpha = torch.mean(gradient, dim=(1, 2), keepdim=True)  # [C, 1, 1]\n",
    "    weights = alpha / (torch.sum(alpha, dim=0) + 1e-5)\n",
    "\n",
    "    activation = activations[0]  # [C, H, W]\n",
    "    grad_cam_map = torch.sum(activation * weights, dim=0)  # [H, W]\n",
    "    grad_cam_map = F.relu(grad_cam_map).detach()\n",
    "\n",
    "    return grad_cam_map\n",
    "\n",
    "# For visualization\n",
    "# def show_cam_on_image(img, mask):\n",
    "#     heatmap = cv2.applyColorMap(np.uint8(255 * mask.cpu().numpy()), cv2.COLORMAP_JET)\n",
    "#     heatmap = np.float32(heatmap) / 255\n",
    "#     cam = heatmap + np.float32(img.permute(1,2,0).cpu().numpy())\n",
    "#     cam = cam / np.max(cam)\n",
    "#     return np.uint8(255 * cam)\n",
    "\n",
    "def show_cam_on_image(img, mask):\n",
    "    # Ensure mask is a 2D tensor and convert it to 0-255 uint8 numpy array\n",
    "    assert len(mask.shape) == 2, \"Mask should be 2D\"\n",
    "    mask_np = np.uint8(255 * mask.cpu().numpy())\n",
    "    \n",
    "    # Convert tensor to numpy array with values in range [0, 255]\n",
    "    img_np = img.squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "    assert img_np.max() <= 255.0 and img_np.min() >= 0.0, \"Image should have values between 0 and 255\"\n",
    "    \n",
    "    # Apply color map\n",
    "    heatmap = cv2.applyColorMap(mask_np, cv2.COLORMAP_JET)\n",
    "    \n",
    "    # Add CAM heatmap to the original image\n",
    "    combined = np.float32(heatmap) + img_np\n",
    "    combined = combined / combined.max() * 255  # Normalize to [0, 255] range\n",
    "    \n",
    "    return np.uint8(combined)\n",
    "\n",
    "# # Example usage:\n",
    "# model = ...  # Your trained model\n",
    "# model.eval()\n",
    "\n",
    "# image = ...  # Your input image of shape [3, 255, 255] and preferably normalized\n",
    "# image_tensor = image.unsqueeze(0)  # make it [1, 3, 255, 255]\n",
    "\n",
    "# cam = grad_cam(model, image_tensor)\n",
    "\n",
    "# # For visualization\n",
    "# result = show_cam_on_image(image, cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Galaxy10DECals('../../data/Galaxy10_DECals.h5') ## removed transform\n",
    "test_loader = DataLoader(data, batch_size=1, shuffle=True)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    # transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)\n",
    "])\n",
    "\n",
    "images, labels = next(iter(test_loader))\n",
    "images = images.permute(0,3,1,2).to(torch.float32)\n",
    "images = F.interpolate(images, size=(255, 255), mode='bilinear', align_corners=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 255, 255]), torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, images.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(images.flatten())\n",
    "# converted_img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "# converted_img = cv2.cvtColor(np.array(images), cv2.COLOR_BGR2BGR555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneralCNN(\n",
       "  (block1): ConvBlock(\n",
       "    (conv): Conv2d(3, 12, kernel_size=(3, 3), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (block2): ConvBlock(\n",
       "    (conv): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block3): ConvBlock(\n",
       "    (conv): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (block4): ConvBlock(\n",
       "    (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block5): ConvBlock(\n",
       "    (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (block6): ConvBlock(\n",
       "    (conv): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (block7): ConvBlock(\n",
       "    (conv): Conv2d(48, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block8): ConvBlock(\n",
       "    (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (block9): ConvBlock(\n",
       "    (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (block10): ConvBlock(\n",
       "    (conv): Conv2d(96, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (block11): ConvBlock(\n",
       "    (conv): Conv2d(112, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (act): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fully_net): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ELU(alpha=1.0, inplace=True)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ELU(alpha=1.0, inplace=True)\n",
       "    (6): Linear(in_features=32, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_str = 'CNN'\n",
    "model_3 = cnn.load_CNN()\n",
    "model_3.load_state_dict(torch.load(f'../../data/new_icml/new_results/CNN.pt', map_location=device))\n",
    "model_3.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([192, 4, 4])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mask should be 2D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb Cell 6\u001b[0m in \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(cam\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# For visualization\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m result \u001b[39m=\u001b[39m show_cam_on_image(images, cam)\n",
      "\u001b[1;32m/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb Cell 6\u001b[0m in \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb#W4sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_cam_on_image\u001b[39m(img, mask):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb#W4sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39m# Ensure mask is a 2D tensor and convert it to 0-255 uint8 numpy array\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb#W4sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(mask\u001b[39m.\u001b[39mshape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mMask should be 2D\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb#W4sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     mask_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39muint8(\u001b[39m255\u001b[39m \u001b[39m*\u001b[39m mask\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/snehpandya/Projects/GCNNMorphology/src/notebooks/gradcam.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39m# Convert tensor to numpy array with values in range [0, 255]\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Mask should be 2D"
     ]
    }
   ],
   "source": [
    "cam = grad_cam(model_3, images)\n",
    "print(cam.shape)\n",
    "\n",
    "# For visualization\n",
    "result = show_cam_on_image(images, cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
